# 13일차

# 1. 수업 내용 정리

### 물체 탐지를 위한 딥러닝 모델

![https://www.gstatic.com/aihub/tfhub/detection/fasterrcnn_output.png](https://www.gstatic.com/aihub/tfhub/detection/fasterrcnn_output.png)

[TensorFlow Hub](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)

[Google Colaboratory](https://colab.research.google.com/drive/1sjHGnIdYLwdgyeGpDhomhZPXJN0WKehm#scrollTo=0XTphzL35BdF)

### 시간 연속적인 프로세스

### (1) Markov Process

[http://www.ktword.co.kr/test/view/view.php?m_temp1=4312](http://www.ktword.co.kr/test/view/view.php?m_temp1=4312)

지금의 상태가 미래를 측정하는 기준이 되기 때문에, 이미 현재의 상태에 과거의 확률이 축적 되어 있다고 가정한다.

[Markov Chains](https://setosa.io/blog/2014/07/26/markov-chains/)

### (2) Hidden Markov Models

Hidden Markov model (HMM)은 Markov model에 은닉된 state와 직접적으로 확인 가능한 observation을 추가하여 확장한 것이다. HMM은 observation을 이용하여 간접적으로 은닉된 state를 추론하기 위한 문제를 풀기 위해 사용된다.

[[머신 러닝] - 은닉 마르코프 모델 (Hidden Markov Model, HMM)](https://untitledtblog.tistory.com/97)

### (3) Kalman Filter

[Kalman filter 소개](https://medium.com/@celinachild/kalman-filter-%EC%86%8C%EA%B0%9C-395c2016b4d6)

# 2. 딥러닝 용어 정리 (2)

## activation 용어 정리

[[모두를 위한 cs231n] Lecture 6. Activation Functions에 대해 알아보자](https://deepinsight.tistory.com/113)

- 전반적인 내용
    
     
    
    [https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvnQPJ%2FbtqEd7HdyUh%2FETBAZp5B17K8KiwAleT3HK%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvnQPJ%2FbtqEd7HdyUh%2FETBAZp5B17K8KiwAleT3HK%2Fimg.png)
    
- sigmoid
    - 특징
        - 출력 값을 0에서 1로 변경해줍니다.(Squashes number to range [0, 1])
        - 가장 많이 사용되었던 활성화 함수라고 합니다.
    - 단점
        - Saturation(포화상태)
        - **Sigmoid outputs are not zero-centered**
        - Activation Function의 구간에서 기울기(gradient)가 0에 가까워지는 현상을 Saturated라고 함.
        - W의 gradient가 항상 Positive or Negative인 경우는 **비효율적**으로 최적해를 탐색하게 됩니다.
    
    [https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fde9qfI%2FbtqEeqfuVyY%2FGQl00vB6WW0K9MLYBZ2Zxk%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fde9qfI%2FbtqEeqfuVyY%2FGQl00vB6WW0K9MLYBZ2Zxk%2Fimg.png)
    
- relu
    - 특징
        - 양의 값에서는 **Saturated** 되지 않습니다.
        - 계산 효율이 뛰어납니다. sigmid/tanh보다 훨씬 빠릅니다.(6배 정도)
        - 생물학적 타당성도 가장 높은 activation function이라고 합니다.
    - 단점
        - zero-centerd가 아니라는 문제가 다시 발생했습니다.(non-zero centered)
        - 또한 음수 영역에서 saturated 되는 문제가 다시 발생합니다.
        - Activation Function의 구간에서 기울기(gradient)가 0에 가까워지는 현상을 **Saturated**라고 함.
    
    [https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FSgcXq%2FbtqEdxTHmfm%2FKrNWz7PrKj1q6viRZp0zSk%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FSgcXq%2FbtqEdxTHmfm%2FKrNWz7PrKj1q6viRZp0zSk%2Fimg.png)
    
- softmax
    - Softmax function은 Neural network의 최상위층에 사용하여 Classification을 위한 function으로 사용한다.
    - Softmax function을 사용하는 주된 이유는 결과를 확률값으로 해석하기 위함이다.
    - 따라서 Softmax의 사용은 normalization의 효과를 얻게 된다.
    
    [https://t1.daumcdn.net/cfile/tistory/2258F43A57FE2F2F27](https://t1.daumcdn.net/cfile/tistory/2258F43A57FE2F2F27)
    

## Optimizer

[[머신러닝 공부]딥러닝/Optimizer정리](https://dbstndi6316.tistory.com/297)

- Optimizer란?
    1. 딥러닝의 학습은 최대한 틀리지 않은 방향으로 해야함.
    2. 얼마나 틀리는지 알게하는 함수가 바로, loss function = 손실함수
    3. 손실함수의 최솟값을 찾는 것을 학습의 목표로 함.
    4. 최솟값을 찾아가는 과정 = Optimization
    5. 최적화를 수행하는 알고리즘 = Optimizer
- Optimizer의 종류
    
    [https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7WJq6%2Fbtq9PTkDVaA%2FIAszauwWN8LojZ1u61VfdK%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7WJq6%2Fbtq9PTkDVaA%2FIAszauwWN8LojZ1u61VfdK%2Fimg.png)
    
    [https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwB6fB%2Fbtq9Y4Zo3cR%2F2N4VZHZWFH2QkEfkQdhhmk%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwB6fB%2Fbtq9Y4Zo3cR%2F2N4VZHZWFH2QkEfkQdhhmk%2Fimg.png)
    
    1. GD = Gradient Descent(경사하강법)
        1. BGD (Batch Gradient Descent)
        2. SGD (Stochastic Gradient Descent)
        3. Mini-batch Gradient Descent
            1. batch_size = 32 등
            2. 설정해 준 mini-batch 만큼의 훈련 데이터를 목적 함수의 계산에 이용
    2. Adam
        1. 현재 가장 많이 사용되는 옵티마이저.
        
        ![https://blog.kakaocdn.net/dn/bZzrdA/btq9Muy3UxE/COPts93KkQbCdKatHjMm1k/img.gif](https://blog.kakaocdn.net/dn/bZzrdA/btq9Muy3UxE/COPts93KkQbCdKatHjMm1k/img.gif)
        
    
    1. Adagrad
        1. Ada = 상황에 맞게 변화하는
        2. 수렴지점으로 빠르게 나아가기 위해서 업데이트 빈도 수에 따라서 lr을 다르게 해줌.
    
    1. Adadelta
        1. Adagrad의 lr 소실 문제를 해결하기 위해 나옴.
        2. 이를 위해 이전의 모든 gradient 정보를 저장하지 않고 지난 x개의 gradient 정보만을 저장
    
    1. RMSProp
        1. Adagrad의 lr 소실 문제를 해결하기 위해 나옴.
        2. γ 값으로 0.9를, η값으로 0.001을 제안한 알고리즘
        

## loss 용어 정리

[[머신러닝] 손실함수의 종류](https://velog.io/@rcchun/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98%EC%9D%98-%EC%A2%85%EB%A5%98)

- binary_crossentropy
    - 활성화 함수 : sigmoid 사용 (출력 값이 0과 1 사이의 값)
    - 실제 레이블과 예측 레이블 간의 교차 엔트로피 손실을 계산한다. **2개의 레이블 클래스(0, 1로 가정)가 있을 때** Binary Crossentropy를 사용하면 좋다.
    
    ```python
    import tensorflow as tf
    import keras
    
    tf.keras.losses.BinaryCrossentropy(from_logits=False, 
    label_smoothing=0, reduction="auto", name="binary_crossentropy")
    ```
    
- categorical_crossentropy
    - **레이블 클래스가 2개 초과일 경우** binary가 아닌 categorical을 사용한다. 보통 softmax 활성화함수 다음에 연계되어 사용되므로 softmax loss로도 불린다.
    
    ```python
    tf.keras.losses.CategoricalCrossentropy(from_logits=False, 
    label_smoothing = 0, reduction="auto", name = 'catogorical_crossentropy")
    ```
    
- Sparse categorical crossentropy
    - one-hot encoding 된 상태일 필요 없이 정수 인코딩 된 상태에서 수행 가능
    
    라벨이 (1,2,3,4) 이런 식으로 정수 형태일 때 사용!
    
    ```python
    tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, 
    from_logits=False, axis = 1)
    ```
    
- Focal loss
    - Focal loss는 페이스북의 Lin et al. 이 소개했습니다.RetinaNet 모델을 학습 시키는데 Focal loss가 한 단계 객체 탐색기를 향상 시킵니다.
    - Focal loss는 **분류 에러에 근거한 loss에 가중치를 부여하는데, 샘플이 CNN에 의해 이미 올바르게 분류되었다면 그것에 대한 가중치는 감소합니다**. 즉, 좀 더 문제가 있는 loss에 더 집중하는 방식으로 불균형한 클래스 문제를 해결하였습니다.
    - Focal loss는 Sigmoid activation을 사용하기 때문에, Binary Cross-Entropy loss라고도 할 수 있습니다.
    
    ```python
    from keras import backend as K
    import tensorflow as tf
    
    def focal_loss(gamma=2, alpha=.25):
        def focal_loss_fixed(y_ture, y_pred):
        	pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
            pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
            return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) *K.pow(pt_0, gamma) * K.log(1. - pt_0))
        return focla_loss_fixed
    ```
    

## TimeseriesGenerator

- 시간 순서가 있는 데이터의 배치를 생성하는 도구 클래스입니다.
    
    [keras-docs-ko/sequence.md at master · keras-team/keras-docs-ko](https://github.com/keras-team/keras-docs-ko/blob/master/sources/preprocessing/sequence.md)
    

## SimpleRNN

- RNN
    
    (Recurrent Neural Network)은 자연어, 주가와 같은 순차 데이터를 모델링하는 데 사용되는 신경망 입니다.
    
    종류
    
    - **SimpleRNN**
        - 이전 timestep의 출력이 다음 timestep으로 완전히 연결된 모델입니다.
    - **LSTM**
        - 기존의 **RNN**이 가진 **vanishing gradient**, 장기 기억 의존성 등의 문제를 해결하기 위해 나온 모델입니다.
        - [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)
    - **GRU**
        - **LSTM**을 좀 더 단순화시키고, 보다 적은 데이터로도 학습이 가능한 모델입니다.
        - [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machie Translation](https://arxiv.org/abs/1406.1078)
    
    출처:
    
    [https://hdevstudy.tistory.com/127](https://hdevstudy.tistory.com/127)
