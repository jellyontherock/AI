# 20일차

## 1. 수업 내용

- VAE
- GAN
    - GAN 관련 내용
    
    [딥러닝 GAN 튜토리얼 - 시작부터 최신 트렌드까지 GAN 논문 순서](https://ysbsb.github.io/gan/2020/06/17/GAN-newbie-guide.html)
    

## 2. 추가 공부해야 할 내용

- 데이콘

[데이터사이언티스트 AI 컴피티션](https://dacon.io/)

- 캐글

[Kaggle: Your Machine Learning and Data Science Community](https://www.kaggle.com/)

- 데이콘에서 흥미로운 주제 발견하여 [교육]의 컨텐츠들 공부 예정
    
    [베이스라인코드](https://dacon.io/competitions/open/235610/codeshare/4154?page=1&dtype=recent)
    
    - 따릉이 이용 예측 분석
        - 실습 파일
        
        [Google Colaboratory](https://colab.research.google.com/drive/1ZCbn77cfjqm8qE02iI8OAYgZ_OStCh98?usp=sharing)
        
        - 생각보다 엄청 간단한 모델이었다.  군더더기 없는 완벽한 예제다. 하지만 처음 알게 된 개념이 있어 정리해보려고 한다.
        - RandomForestRegressor
            - scikit learn 설명과 예제
            
            [sklearn.ensemble.RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
            
            [점프 투 파이썬](https://wikidocs.net/26294)
            
            ```python
            from sklearn.ensemble import RandomForestRegressor # import 시켜준 모듈
            
            #예시
            estimator = RandomForestRegressor(n_estimators=10, 
            bootstrap=True, criterion='mse', max_depth=None, 
            max_leaf_nodes=None, min_samples_split=2, min_samples_leaf=1, 
            max_features=’auto’)
            
            # n_estimators => 생성할 tree 개수
            # max_features => 최대 선택할 특성의 수
            
            #랜덤 포레스트 회귀
            
            '''
            random forest의 Idea는
            
            각각의 tree는 비교적 예측을 잘 할 수 있지만 데이터의 일부에 overfitting하는 경향을 가진다는 데 기초합니다.
            
            잘 작동하되 서로 다른 방향으로 overfitting된 tree를 많이 만들면 그 결과를 평균냄으로써 overfitting된 양을 줄일 수 있습니다.
            
            이렇게 하면 tree model의 예측 성능이 유지되면서 overfitting이 줄어드는 것이 수학적으로 증명되었습니다.
            '''
            
            # https://woolulu.tistory.com/28
            ```
            
            ```python
            # 전체 코드
            # 내 파일과 약간 차이 있음
            
            import pandas as pd
            from sklearn.ensemble import RandomForestRegressor
            
            train = pd.read_csv('/content/drive/MyDrive/머신러닝/따릉이/train.csv') 
            test = pd.read_csv('/content/drive/MyDrive/머신러닝/따릉이/test.csv')
            
            train.info()
            train.head()
            test.head()
            
            # 결측값 확인 및 처리
            
            train.isnull().sum() #결측값 처리
            test.isnull().sum()
            train.fillna(0,inplace = True)
            test.fillna(0,inplace = True)
            
            # 모델 정의 및 학습
            
            train_x = train.drop(['count'],axis = 1) # count -> 시간에 따른 따릉이 대여 수
            train_y = train['count']
            
            model=RandomForestRegressor(n_estimators=100) # 그냥 100이라고 해도 실행됨
            model.fit(train_x,train_y)
            
            # 학습된 모델로 예측 데이터 생성
            
            pred = model.predict(test)
            
            # 제출 데이터 생성
            
            submission = pd.read_csv('/content/drive/MyDrive/머신러닝/따릉이/submission.csv')
            submission
            
            submission['count'] = pred
            submission
            
            submission.to_csv('베이스라인.csv',index = False)
            ```
            

## 3. 오늘 읽은 논문

- 자연어처리 기술을 활용한 유튜브 악성 댓글 자동 블라인드 시스템 / 이재은 외 4인
    
    [자연어처리 기술을 활용한 유튜브 악성 댓글 자동 블라인드 시스템 (1).pdf](20%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%2034dae/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EA%B8%B0%EC%88%A0%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%9C%A0%ED%8A%9C%EB%B8%8C_%EC%95%85%EC%84%B1_%EB%8C%93%EA%B8%80_%EC%9E%90%EB%8F%99_%EB%B8%94%EB%9D%BC%EC%9D%B8%EB%93%9C_%EC%8B%9C%EC%8A%A4%ED%85%9C_(1).pdf)
    
    [https://github.com/commentcover/COmmentCOver](https://github.com/commentcover/COmmentCOver)
    
- 생성적 적대 신경망(GAN)을 이용한 한국어 문서에서의 문맥의존 철자오류 교정 / 이정훈, 권혁철
    
    [Context-Sensitive Spelling Error Correction Techniques in Korean Documents using Generative Adversarial Network.pdf](20%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%8E%E1%85%A1%2034dae/Context-Sensitive_Spelling_Error_Correction_Techniques_in_Korean_Documents_using_Generative_Adversarial_Network.pdf)
    
    ⇒ 100% 이해가는건 아니라 다시 츄라이