# 18일차

## 1. 수업 내용

1) 한국어 임베딩

2) 네이버 영화 리뷰 / 감정분석

## 2. 셀프 Q&A

1. conv2d MaxPooling2D 64 128 ... 로 하는 이유 

- 분석하는 사람이 임의로 지정하는 것.
    - 연산을 위해 주로 2의 배수로 함.
- 커널 사이즈 역시 임의이다. 특징을 뽑아내는 크기.
- 나중에 점점 줄여가는게 cnn 구조의 특징.
- parameter 수가 많다고 단연 좋은 것은 아님.
- 이미지에서 pooling은 거의 maxpooling을 쓰는 편이다.
- 예시 코드
    
    ```python
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size= (3,3), activation = "relu", input_shape=(28, 28,1)))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size= (3,3), activation = "relu"))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(units=128, activation="relu"))
    model.add(tf.keras.layers.Dense(units=64, activation="relu"))
    model.add(tf.keras.layers.Dense(units=10, activation="softmax"))
    ```
    

2. TimeseriesGenerator에서 s_train, s_train 인 이유?

    ```python
    from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
    length = 30
    generator = TimeseriesGenerator(s_train, s_train, length=length, batch_size = 1)
    ```

[keras-docs-ko/sequence.md at master · keras-team/keras-docs-ko](https://github.com/keras-team/keras-docs-ko/blob/master/sources/preprocessing/sequence.md)

     **data**: 리스트 또는 NumPy 배열과 같이 인덱싱 가능한 2D 데이터로 0번째 축axis은 연속된 시점에 모인 표본sample들로 이루어진 시간 차원을 나타냅니다.

     **targets**: `data`의 시간 단계와 상응하는 목표값으로 0번째 축의 길이가 `data`와 서로 같아야 합니다.

3. 람다 어떻게 써요?
    1. 함수를 한 줄로 만들어 줄 때에 씁니다!
    
    ```python
    >>> def hap(x, y):
    ...   return x + y
    ...
    >>> hap(10, 20)
    30
    ```
    
    ```python
    >>> (lambda x,y: x + y)(10, 20)
    30
    ```
    
    [점프 투 파이썬](https://wikidocs.net/64)
    
4. 임베딩 공부한거 요약 좀 해보시겠어요?
    1. 원핫코딩
        
        ```python
        '''
        삼성전자 엘지전자 지금  풀매수  타이밍 했는데 망했다.    => vo_size => 7개
        [  1    ,  0    , 1  ,  1   ,   1   ,  0  , 0     ]
        [  0    ,  1    , 0  ,  1   ,   0   ,  1  , 1     ]]
        '''
        
        # 원핫 -> 정수 -> 임베딩
        #  7   ->   4  ->   8
        word_1 = "삼성전자 지금 풀매수 타이밍" # input_length 4개
        word_1_onehot = [1, 0, 1, 1, 1, 0, 0]
        #int_encoding
        word_1_intenco = np.array([0, 2, 3, 4])
        
        word_2 = "엘지전자 풀매수 했는데 망했다"
        word_2_onehot = [0,1,0,1,0,1,1]
        word_2_intenco = np.array([1, 3, 5, 6])
        
        batch_word_int = np.array([word_1_intenco, word_2_intenco]) #out_size=> 2
        ```
        
        원핫코딩 방식으로 vo_size, out_size, input_length를 정리하고 이를 embedding 할 때에 반영합니다.  (원핫코딩 → 정수 → 임베딩)
        
    2. embedding 형태
        
        ```python
        vo_size = 7 #단어 7개
        out_size = 2
        input_length = 4 # 입력할 단어의 길이 
        
        model_embedding2 = tf.keras.Sequential()
        model_embedding2.add(tf.keras.layers.Embedding(input_dim = vo_size, output_dim = out_size, input_length = input_length))
        ```
        
    3. bidirectional RNN
        
        ```python
        # bidirectional RNN
        # 기존의 순방향 RNN 에서 역방향으로 학습할수 있도록 감싸준다. 
        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional
        
        # Bidirectional 에서 return_sequences=True 가 아니면 loss->0 이 되지 않을수 있다. 
        # output 에 영향을 받지 않아서...
        
        model_bid = tf.keras.Sequential()
        model_bid.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=False), input_shape=(10,1)))
        # model_bid.add(tf.keras.layers.Dense(1))
        model_bid.summary()
        # 순방향과 역방향 LSTM 모델이 생성되어 두배의 unit 이 생성된다.
        # 순방향과 역방향 모두 학습.
        ```
        
    
    4. 자연어 처리
    
        1. Tokenization
            1. Token ⇒ 문장을 잘라낸 조각
            2. 문장 ⇒ 단어 ⇒ 벡터로 쪼개는 과정
        2. 자연어 처리 과정
            1. 은는이가 (stop words) 제거  / 형태소 분석기 사용
            2. Tokenization
            3. word2vec or onehot (단어를 숫자로 변경)
            4. word Encoding
            5. 문장 길이 변경
        3. 한글 형태소 분석기 모듈 (꼬꼬마, Okt, 한나눔)
        
            ```python
            ##kkma 
            from konlpy.tag import Kkma
            kkma = Kkma()
            text = "그녀는 엽떡을 좋아한다. 하지만 나는 맵찔이다"
        
            # 문장 분석, 마침표 여부에 상관없이 문장을 구분해준다. 
            kkma.sentences(text)
            # ['그녀는 엽 떡을 좋아한다 하지만 나는 맵찔이다']
        
            # 명사 분석
            kkma.nouns(text)
            #   ['그녀', '엽', '엽떡', '떡', '나', '맵찔']
        
            # 형태소 분석 : 형태소(최소한의 의미 단어)
            # 맥북 -> 맥, 북, 한국어 -> 한국 어
            kkma.pos(text)
        
            '''
            [('그녀', 'NP'),
            ('는', 'JX'),
            ('엽', 'NNM'),
            ('떡', 'NNG'),
            ('을', 'JKO'),
            ('좋아하', 'VV'),
            ('ㄴ다', 'EFN'),
            ('.', 'SF'),
            ('하지만', 'MAC'),
            ('나', 'NP'),
            ('는', 'JX'),
            ('맵찔', 'UN'),
            ('이다', 'JC')]
            '''
            # 품사 태그가 달리지 않는 형태소 분석
            hannanum.morphs(text)
        
            # 한나눔
            from konlpy.tag import Hannanum
            hannanum = Hannanum()
        
            #sentences , nouns, pos, morphs 사용 동일
        
            #OKT (open korean text) 트위터에서 개발 프로젝트 -> 오픈소스 프로젝트로 변경
            from konlpy.tag import Okt
            tk =Okt()
            ```
        
    
    5. tokenization 실습
        
        ```python
        tk =Okt()
        def word_tokenization(text):
            stop_words = ["는", "을", "를", '이', '가', '의', '던', '고', '하', '다', '은', '에', '들', '지', '게', '도'] # 한글 stop words
            return [word for word in okt.morphs(text) if word not in stop_words]
            # okt.morphs 를 통해 혐태소 분석을 해주고 stop_words에 없으면 list에 추가후 반환
        
        data =word_tokenization(text)
        data
        # ['그녀', '엽떡', '좋아한다', '.', '하지만', '나', '맵', '찔이다']
        ```
        
        ```python
        from tensorflow.keras.preprocessing.text import Tokenizer
        token = Tokenizer()
        token.fit_on_texts(data)
        
        token.word_index
        # {'그녀': 1, '나': 5, '맵': 6, '엽떡': 2, '좋아한다': 3, '찔이다': 7, '하지만': 4}
        ```
        
    
    6. RNN을 이용한 네이버 영화 분석
      1. 멀티 프로세싱
            
            ```python
            def data_preprocessing(csv_path):
              data_set = pd.read_csv(csv_path) #csv 파일 불러오기
              data_set = data_set.dropna()
              train = pd.DataFrame() # data_set 에 필요한 데이터만 추출해 train df 에 저장한다. 
              train["document"] = data_set["감상평"]
              train["label"] = data_set["평점"]
              return train
            ```
            
      2. 00% 확률으로 긍정 리뷰입니다!
            
            ```python
            def score_review():
              test_input = input("리뷰를 입력하세요 : ")# 단어입력
              tmp = word_tokenization(test_input)# 단어 -> token
              word_vec = [list(np.array(tokenizer.texts_to_sequences(tmp))[:, 0])]# token -> vec
              test_padded = pad_sequences(word_vec, truncating=trun, padding= padd, maxlen=max_length)# padding 추가
              result = model.predict(test_padded)# predict
              print(f"{result[0][1]*100}의 확률로 긍정 리뷰 입니다.")# print
            ```
            
      3. 멀티 프로세싱으로 학습시키기 후 토크나이저 처리
            
            ```python
            train = data_preprocessing("2022-04-08 06:49:12.120233.csv")
            data = train["document"].apply((lambda x : word_tokenization(x))) # 감상평 적용
            train_labels = train["label"].apply(score) # 1, 0  적용
            
            train_sequences = tokenizer.texts_to_sequences(data)# tokenizer 를 이용해 토큰 으로 변경
            train_padded = pad_sequences(train_sequences, truncating=trun, padding= padd, maxlen=max_length)# padding 처리 
            train_labels = np.array(train_labels)
            
            score_review() # :) 확률 나온다!
            ```
            
        
    
    ## 3. 참고문헌
    
    [day18-RNN.pdf의 사본](https://drive.google.com/file/d/1quVeY_Ebap1sIdfUh1yzpa6ARCBGSN61/view?usp=drivesdk)
